{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langchain_core.messages import HumanMessage, ToolMessage, SystemMessage\n",
    "\n",
    "sys.path.append(str(Path().cwd().parent))\n",
    "\n",
    "from llm_experiments.models import instantiate_chat\n",
    "\n",
    "\n",
    "@tool\n",
    "def round_number(n: float, precision: int) -> int:\n",
    "    \"\"\"add two numbers together\"\"\"\n",
    "    return round(n, precision)\n",
    "\n",
    "\n",
    "model = instantiate_chat(\"4o-mini\")\n",
    "tools = [round_number]\n",
    "tools_by_name = {t.name: t for t in tools}\n",
    "tooled_model = model.bind_tools(tools)\n",
    "\n",
    "\n",
    "def llm_call(state: MessagesState):\n",
    "    role = \"you are a helpful assistant\"\n",
    "    messages = [tooled_model.invoke([SystemMessage(content=role), *state[\"messages\"]])]\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "\n",
    "def tool_node(state: MessagesState):\n",
    "    results = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool = tools_by_name[tool_call[\"name\"]]\n",
    "        observation = tool.invoke(tool_call[\"args\"])\n",
    "        results.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n",
    "    return {\"messages\": results}\n",
    "\n",
    "\n",
    "def should_continue(state: MessagesState):\n",
    "    if state[\"messages\"][-1].tool_calls:\n",
    "        return \"action\"\n",
    "    else:\n",
    "        return END\n",
    "\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "builder.add_node(\"llm_call\", llm_call)\n",
    "builder.add_node(\"environment\", tool_node)\n",
    "\n",
    "builder.add_edge(START, \"llm_call\")\n",
    "builder.add_conditional_edges(\"llm_call\", should_continue, {\"action\": \"environment\", END: END})\n",
    "builder.add_edge(\"environment\", \"llm_call\")\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"round 1.234 to 2 decimal places\"\n",
    "messages = graph.invoke({\"messages\": [HumanMessage(content=query)]})\n",
    "for index, m in enumerate(messages[\"messages\"]):\n",
    "    print(index, m.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
